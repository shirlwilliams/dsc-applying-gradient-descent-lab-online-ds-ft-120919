{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Gradient Descent - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the last lesson, we derived the functions that we help us descend along our cost functions efficiently.  Remember that this technique is not so different from what we saw with using the derivative to tell us our next step size and direction in two dimensions.  \n",
    "\n",
    "![](./images/slopes.png)\n",
    "\n",
    "When descending along our cost curve in two dimensions, we used the slope of the tangent line at each point, to tell us how large of a step to take next.  And with the cost curve being a function of $m$ and $b$, we had to use the gradient to determine each step.  \n",
    "\n",
    "<img src=\"./images/new_gradientdescent.png\" width=\"600\">\n",
    "\n",
    "But really it's an analogous approach.  Just like we can calculate the use derivative of a function $f(x)$ to calculate the slope at a given value of $x$ on the graph and thus our next step.  Here, we calculated the partial derivative with respect to both variables, our slope and y-intercept, to calculate the amount to move next in either direction and thus to steer us towards our minimum.   \n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "* Create functions to perform a simulation of gradient descent for an actual dataset\n",
    "* Represent RSS as a multivariable function and take partial derivatives to perform gradient descent\n",
    "\n",
    "## Reviewing our gradient descent formulas\n",
    "\n",
    "Luckily for us, we already did the hard work of deriving these formulas.  Now we get to see the fruit of our labor.  The following formulas tell us how to update regression variables of $m$ and $b$ to approach a \"best fit\" line.   \n",
    "\n",
    "- $ \\frac{dJ}{dm}J(m,b) = -2\\sum_{i = 1}^n x_i(y_i - (mx_i + b)) = -2\\sum_{i = 1}^n x_i*\\epsilon_i$ \n",
    "- $ \\frac{dJ}{db}J(m,b) = -2\\sum_{i = 1}^n(y_i - (mx_i + b)) = -2\\sum_{i = 1}^n \\epsilon_i $\n",
    "\n",
    "Now the formulas above tell us to take some dataset, with values of $x$ and $y$, and then given a regression formula with values $m$ and $b$, iterate through our dataset, and use the formulas to calculate an update to $m$ and $b$.  So ultimately, to descend along the cost function, we will use the calculations:\n",
    "\n",
    "`current_m` = `old_m` $ -  (-2*\\sum_{i=1}^n x_i*\\epsilon_i )$\n",
    "\n",
    "`current_b` =  `old_b` $ - ( -2*\\sum_{i=1}^n \\epsilon_i )$\n",
    "\n",
    "Ok let's turn this into code.  First, let's initialize our data like we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float_kind':'{:f}'.format})\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(225)\n",
    "\n",
    "x = np.random.rand(30, 1).reshape(30)\n",
    "y_randterm = np.random.normal(0,3,30)\n",
    "y = 3 + 50* x + y_randterm\n",
    "\n",
    "data = np.array([y, x])\n",
    "data = np.transpose(data)\n",
    "\n",
    "plt.plot(x, y, '.b')\n",
    "plt.xlabel(\"x\", fontsize=14)\n",
    "plt.ylabel(\"y\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now\n",
    "\n",
    "- Let's set our initial regression line by initializing $m$ and $b$ variables as zero.  Store them in `b_current` and `m_current`.\n",
    "- Let's next initialize updates to these variables by setting the variables, `update_to_b` and `update_to_m` equal to 0.\n",
    "- Define an `error_at` function which returns the error $\\epsilon_i$ for a given $i$. The parameters are:\n",
    "> point: a row of the particular data set  \n",
    "> $b$: the intercept term  \n",
    "> $m$: the slope  \n",
    "\n",
    "- Them, use this `error_at` function to iterate through each of the points in the dataset, and at each iteration change our `update_to_b` by $2*\\epsilon$ and change our `update_to_m` by $2*x*\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial variables of our regression line\n",
    "b_current = 0\n",
    "m_current = 0\n",
    "\n",
    "#amount to update our variables for our next step\n",
    "update_to_b = 0\n",
    "update_to_m = 0\n",
    "\n",
    "# Define the error_at function\n",
    "def error_at(point, b, m):\n",
    "    return (point[0] - (m * point[1] + b))\n",
    "\n",
    "# iterate through data to change update_to_b and update_to_m\n",
    "for i in range(0, len(data)):\n",
    "    update_to_b += 2 * (error_at(data[i],b_current, m_current))\n",
    "    update_to_m += 2 * (error_at(data[i],b_current, m_current))  * data[i][1]\n",
    "\n",
    "# Create new_b and new_m by subtracting the updates from the current estimates\n",
    "new_b = b_current - update_to_b\n",
    "new_m = m_current - update_to_m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last two lines of the code above, we calculate our `new_b` and `new_m` values by updating our taking our current values and adding our respective updates.  We define a function called `error_at`, which we can use in the error component of our partial derivatives above.\n",
    "\n",
    "The code above represents **just one** update to our regression line, and therefore just one step towards our best fit line.  We'll just repeat the process to take multiple steps.  But first, we have to make a couple of other changes. \n",
    "\n",
    "## Tweaking our approach \n",
    "\n",
    "Ok, the above code is very close to what we want, but we just need to make tweaks to our code before it's perfect.\n",
    "\n",
    "The first one is obvious if we think about what these formulas are really telling us to do.  Look at the graph below, and think about what it means to change each of our $m$ and $b$ variables by at least the sum of all of the errors, of the $y$ values that our regression line predicts and our actual data.  That would be an enormous change.  To ensure that we drastically updating our regression line with each step, we multiply each of these partial derivatives by a learning rate.  As we have seen before, the learning rate is just a small number, like $.\n",
    "01$ which controls how large our updates to the regression line will be.  The learning rate is  represented by the Greek letter eta, $\\eta$, or alpha $\\alpha$.  We'll use eta, so $\\eta = .01$ means the learning rate is $.01$.\n",
    "\n",
    "Multiplying our step size by our learning rate works fine, so long as we multiply both of the partial derivatives by the same amount.  This is because without gradient,  $ \\nabla J(m,b)$, we think of as steering us in the correct direction.  In other words, our derivatives ensure we are making the correct **proportional** changes to $m$ and $b$.  So scaling down these changes to make sure we don't update our regression line too quickly works fine, so long as we keep me moving in the correct direction.  While we're at it, we can also get rid of multiplying our partials by 2.  As mentioned, so long as our changes are proportional we're in good shape. \n",
    "\n",
    "For our second tweak, note that in general the larger the dataset, the larger the sum of our errors would be.  But that doesn't mean our formulas are less accurate, and there deserve larger changes.  It just means that the total error is larger.  But we should really think accuracy as being proportional to the size of our dataset.  We can correct for this effect by dividing the effect of our update by the size of our dataset, $n$.\n",
    "\n",
    "Make these changes below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#amount to update our variables for our next step\n",
    "update_to_b = 0\n",
    "update_to_m = 0\n",
    "\n",
    "# define learning rate and n\n",
    "learning_rate = .01\n",
    "n = len(data)\n",
    "\n",
    "# create update_to_b and update_to_m\n",
    "for i in range(0, n):\n",
    "    update_to_b += -(1/n*error_at(data[i], b_current, m_current))\n",
    "    update_to_m += -(1/n*error_at(data[i], b_current, m_current))*data[i][1]\n",
    "    \n",
    "# create new_b and new_m\n",
    "new_b = b_current - (learning_rate * update_to_b)\n",
    "new_m = m_current - (learning_rate * update_to_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our code now reflects what we know about our gradient descent process.  Start with an initial regression line with values of $m$ and $b$.  Then for each point, calculate how the regression line fares against the actual point (that is, find the error).  Update what the next step to the respective variable should be by using the partial derivative.  And after iterating through all of the points, update the value of $b$ and $m$ appropriately, scaled down by a learning rate.\n",
    "\n",
    "## Seeing our gradient descent formulas in action\n",
    "\n",
    "As mentioned earlier, the code above represents just one update to our regression line, and therefore just one step towards our best fit line.  To take multiple steps we wrap the process we want to duplicate in a function called `step_gradient` and then can call that function as much as we want. With this function:\n",
    "\n",
    "- Include a learning_rate of 0.1\n",
    "- Return a tuple of (b,m)  \n",
    "The parameters should be:\n",
    "> b_current : the starting value of b   \n",
    "> m_current : the starting value of m   \n",
    "> points : the number of points at which we want to check our gradient \n",
    "\n",
    "See if you can use your `error_at` function within the `step_gradient` function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient(b_current, m_current, points):\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    learning_rate = .1\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i][1]\n",
    "        y = points[i][0]\n",
    "        b_gradient += -(1/N) * (y - (m_current * x + b_current))\n",
    "        m_gradient += -(1/N) * x * (y -  (m_current * x + b_current))\n",
    "    new_b = b_current - (learning_rate * b_gradient)\n",
    "    new_m = m_current - (learning_rate * m_gradient)\n",
    "    return (new_b, new_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's initialize `b` and `m` as 0 and run a first iteration of the `step_gradient` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0250308395837813\n",
      "2.0728619246505193\n"
     ]
    }
   ],
   "source": [
    "b = 0\n",
    "m = 0\n",
    "first_step = step_gradient(b, m, data)\n",
    "\n",
    "print(first_step[0])\n",
    "print(first_step[1])\n",
    "# b= 3.02503, m= 2.07286"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So just looking at input and output, we begin by setting $b$ and $m$ to 0 and 0.  Then from our step_gradient function, we receive new values of $b$ and $m$ of 3.02503 and 2.0728.  Now what we need to do, is take another step in the correct direction by calling our step gradient function with our updated values of $b$ and $m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.634896312558807, 3.902265648903966)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_to_b = first_step[0]\n",
    "update_to_m = first_step[1]\n",
    "step_gradient(update_to_b, update_to_m, data)\n",
    "# b = 5.63489, m= 3.902265"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this, say, 1000 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the initial step with m and b values and the current error\n",
    "b = 0\n",
    "m = 0\n",
    "iterations = []\n",
    "\n",
    "for i in range(1000):\n",
    "    iteration = step_gradient(b, m, data)\n",
    "    b = iteration[0]\n",
    "    m = iteration[1]\n",
    "    # update b and m values\n",
    "    iterations.append(iteration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the estimates in the last iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3.0250308395837813, 2.0728619246505193),\n",
       " (5.634896312558807, 3.902265648903966),\n",
       " (7.884345171519152, 5.5200596577756995),\n",
       " (9.820920294267818, 6.953902303201383),\n",
       " (11.485907009550733, 8.227813233586371),\n",
       " (12.915156619426247, 9.362652253147472),\n",
       " (14.139801542875658, 10.3765351615667),\n",
       " (15.186876343718733, 11.285194867591269),\n",
       " (16.0798570288238, 12.102294978739781),\n",
       " (16.839129372557856, 12.839702121442453),\n",
       " (17.482395607898194, 13.507722422849499),\n",
       " (18.025027595391315, 14.115306870770379),\n",
       " (18.48037351368167, 14.670229647501909),\n",
       " (18.86002418835148, 15.179242994285609),\n",
       " (19.174044370826213, 15.648211695053913),\n",
       " (19.431173580052743, 16.0822298616453),\n",
       " (19.639000512607243, 16.485722349683147),\n",
       " (19.804114499729806, 16.86253282778208),\n",
       " (19.932237031999197, 17.216000256555166),\n",
       " (20.028335974823563, 17.549025302736702),\n",
       " (20.0967247527028, 17.864128012997963),\n",
       " (20.14114848043059, 18.16349789771379),\n",
       " (20.164858759069432, 18.449037423559545),\n",
       " (20.17067862845862, 18.722399782361524),\n",
       " (20.161058971693627, 18.98502168946767),\n",
       " (20.1381274965307, 19.238151865772664),\n",
       " (20.103731270622212, 19.482875771444856),\n",
       " (20.05947365892371, 19.72013708464523),\n",
       " (20.00674639996872, 19.950756353609776),\n",
       " (19.946757460755236, 20.175447194091262),\n",
       " (19.880555225795167, 20.3948303552003),\n",
       " (19.80904950276514, 20.609445934172285),\n",
       " (19.733029763706508, 20.819763983668498),\n",
       " (19.65318098558706, 21.02619372315969),\n",
       " (19.570097406157934, 21.2290915381001),\n",
       " (19.484294469461116, 21.428767926423046),\n",
       " (19.39621919923661, 21.625493530894328),\n",
       " (19.306259207124032, 21.819504377627865),\n",
       " (19.21475051532502, 22.011006425235344),\n",
       " (19.121984349748228, 22.200179515332877),\n",
       " (19.028213039125525, 22.387180803188084),\n",
       " (18.933655137757196, 22.57214773692288),\n",
       " (18.838499874059735, 22.755200644683494),\n",
       " (18.742911013643383, 22.93644498137048),\n",
       " (18.6470302139697, 23.115973279731666),\n",
       " (18.55097993749939, 23.293866844724747),\n",
       " (18.454865981434878, 23.470197224935987),\n",
       " (18.358779674515485, 23.64502749039501),\n",
       " (18.26279978468241, 23.818413342264446),\n",
       " (18.16699417566444, 23.990404076530112),\n",
       " (18.071421245527443, 24.161043420905536),\n",
       " (17.97613117588229, 24.330370261636048),\n",
       " (17.88116701666936, 24.498419274691848),\n",
       " (17.786565628158584, 24.66522147393254),\n",
       " (17.692358498956175, 24.83080468716978),\n",
       " (17.598572456336118, 24.995193969616675),\n",
       " (17.505230283067128, 25.158411962963797),\n",
       " (17.41235125304074, 25.320479207237344),\n",
       " (17.319951596386733, 25.48141441165324),\n",
       " (17.228044903355837, 25.641234689863214),\n",
       " (17.13664247502827, 25.799955764278774),\n",
       " (17.045753627846192, 25.957592143542307),\n",
       " (16.955385958047177, 26.11415727667899),\n",
       " (16.86554557127601, 26.26966368699819),\n",
       " (16.776237281957613, 26.424123088409132),\n",
       " (16.687464786410814, 26.57754648646498),\n",
       " (16.599230813158865, 26.72994426614487),\n",
       " (16.51153725343787, 26.88132626811902),\n",
       " (16.424385274509266, 27.031701855012336),\n",
       " (16.337775418039595, 27.181079968982523),\n",
       " (16.25170768551281, 27.329469181755545),\n",
       " (16.16618161238192, 27.47687773811082),\n",
       " (16.081196332441966, 27.62331359367795),\n",
       " (15.99675063371144, 27.76878444779344),\n",
       " (15.91284300693973, 27.913297772067256),\n",
       " (15.82947168771122, 28.056860835223624),\n",
       " (15.74663469298885, 28.199480724706167),\n",
       " (15.664329852829061, 28.341164365473),\n",
       " (15.582554837903698, 28.481918536351365),\n",
       " (15.501307183380845, 28.621749884272774),\n",
       " (15.42058430964388, 28.760664936667354),\n",
       " (15.340383540264968, 28.89867011225948),\n",
       " (15.260702117594462, 29.03577173047486),\n",
       " (15.181537216280072, 29.17197601964156),\n",
       " (15.102885954988386, 29.307289124143562),\n",
       " (15.024745406565431, 29.44171711066445),\n",
       " (14.947112606841834, 29.575265973640736),\n",
       " (14.869984562261063, 29.707941640028707),\n",
       " (14.793358256485769, 29.83974997347486),\n",
       " (14.717230656116824, 29.970696777968264),\n",
       " (14.641598715641942, 30.10078780104281),\n",
       " (14.566459381715397, 30.230028736588366),\n",
       " (14.491809596856974, 30.358425227322165),\n",
       " (14.417646302646713, 30.485982866964864),\n",
       " (14.34396644248189, 30.61270720216002),\n",
       " (14.270766963953992, 30.738603734170518),\n",
       " (14.198044820895785, 30.863677920381097),\n",
       " (14.125796975142006, 30.987935175632355),\n",
       " (14.054020398041487, 31.11138087340816),\n",
       " (13.982712071753532, 31.234020346895605),\n",
       " (13.91186899035704, 31.355858889934094),\n",
       " (13.841488160797132, 31.47690175786794),\n",
       " (13.771566603690786, 31.597154168315004),\n",
       " (13.70210135401012, 31.716621301862247),\n",
       " (13.633089461659546, 31.8353083026976),\n",
       " (13.56452799196087, 31.9532202791864),\n",
       " (13.49641402605855, 32.070362304399445),\n",
       " (13.428744661255722, 32.18673941659892),\n",
       " (13.361517011290225, 32.3023566196875),\n",
       " (13.2947282065586, 32.417218883625345),\n",
       " (13.228375394295039, 32.53133114481901),\n",
       " (13.162455738711287, 32.64469830648581),\n",
       " (13.096966421102767, 32.75732523899665),\n",
       " (13.031904639925441, 32.869216780200084),\n",
       " (12.967267610847387, 32.98037773572977),\n",
       " (12.903052566778497, 33.090812879297424),\n",
       " (12.839256757881293, 33.200526952973),\n",
       " (12.775877451565423, 33.30952466745358),\n",
       " (12.712911932468103, 33.41781070232232),\n",
       " (12.650357502422429, 33.525389706298576),\n",
       " (12.588211480415275, 33.632266297480236),\n",
       " (12.526471202536223, 33.73844506357906),\n",
       " (12.465134021918814, 33.84393056214986),\n",
       " (12.40419730867521, 33.94872732081411),\n",
       " (12.343658449825243, 34.0528398374786),\n",
       " (12.283514849220676, 34.15627258054959),\n",
       " (12.22376392746539, 34.25902998914291),\n",
       " (12.164403121832144, 34.361116473290416),\n",
       " (12.105429886176431, 34.462536414143074),\n",
       " (12.04684169084791, 34.56329416417097),\n",
       " (11.988636022599826, 34.66339404736055),\n",
       " (11.930810384496773, 34.76284035940921),\n",
       " (11.873362295821096, 34.861637367917524),\n",
       " (11.816289291978206, 34.95978931257919),\n",
       " (11.759588924401049, 35.05730040536891),\n",
       " (11.703258760453902, 35.154174830728294),\n",
       " (11.647296383335698, 35.25041674574987),\n",
       " (11.591699391983012, 35.346030280359365),\n",
       " (11.53646540097284, 35.44101953749633),\n",
       " (11.481592040425289, 35.535388593293085),\n",
       " (11.42707695590626, 35.629141497252206),\n",
       " (11.372917808330232, 35.72228227242249),\n",
       " (11.319112273863192, 35.81481491557351),\n",
       " (11.26565804382579, 35.90674339736878),\n",
       " (11.212552824596758, 35.9980716625376),\n",
       " (11.159794337516665, 36.08880363004556),\n",
       " (11.107380318792014, 36.17894319326383),\n",
       " (11.055308519399732, 36.26849422013712),\n",
       " (11.003576704992085, 36.35746055335055),\n",
       " (10.952182655802025, 36.445846010495195),\n",
       " (10.90112416654902, 36.53365438423255),\n",
       " (10.85039904634534, 36.620889442457795),\n",
       " (10.800005118602863, 36.707554928461974),\n",
       " (10.749940220940369, 36.79365456109299),\n",
       " (10.700202205091363, 36.879192034915576),\n",
       " (10.650788936812422, 36.96417102037009),\n",
       " (10.601698295792065, 37.04859516393031),\n",
       " (10.552928175560181, 37.13246808826014),\n",
       " (10.504476483397971, 37.21579339236924),\n",
       " (10.45634114024846, 37.29857465176766),\n",
       " (10.408520080627541, 37.380815418619385),\n",
       " (10.36101125253557, 37.462519221894915),\n",
       " (10.313812617369512, 37.543689567522804),\n",
       " (10.266922149835638, 37.62432993854019),\n",
       " (10.220337837862761, 37.704443795242355),\n",
       " (10.17405768251603, 37.784034575331255),\n",
       " (10.128079697911271, 37.86310569406313),\n",
       " (10.082401911129862, 37.94166054439509),\n",
       " (10.037022362134163, 38.01970249713076),\n",
       " (9.991939103683483, 38.09723490106495),\n",
       " (9.947150201250592, 38.174261083127426),\n",
       " (9.902653732938754, 38.25078434852567),\n",
       " (9.858447789399316, 38.326807980886734),\n",
       " (9.814530473749816, 38.40233524239819),\n",
       " (9.770899901492623, 38.47736937394811),\n",
       " (9.727554200434104, 38.55191359526415),\n",
       " (9.684491510604317, 38.625971105051754),\n",
       " (9.641709984177224, 38.69954508113138),\n",
       " (9.599207785391414, 38.77263868057492),\n",
       " (9.556983090471357, 38.845255039841135),\n",
       " (9.515034087549155, 38.917397274910286),\n",
       " (9.473358976586812, 38.98906848141783),\n",
       " (9.431955969299, 39.06027173478727),\n",
       " (9.390823289076343, 39.131010090362125),\n",
       " (9.349959170909178, 39.20128658353703),\n",
       " (9.309361861311842, 39.27110422988802),\n",
       " (9.269029618247421, 39.34046602530188),\n",
       " (9.228960711053013, 39.40937494610477),\n",
       " (9.189153420365466, 39.477833949189886),\n",
       " (9.149606038047612, 39.54584597214436),\n",
       " (9.110316867114964, 39.61341393337534),\n",
       " (9.071284221662909, 39.68054073223519),\n",
       " (9.032506426794363, 39.747229249145896),\n",
       " (8.993981818547908, 39.81348234572271),\n",
       " (8.955708743826392, 39.87930286489691),\n",
       " (8.91768556032599, 39.94469363103779),\n",
       " (8.879910636465747, 40.009657450073874),\n",
       " (8.842382351317553, 40.07419710961331),\n",
       " (8.805099094536606, 40.138315379063485),\n",
       " (8.768059266292296, 40.20201500974987),\n",
       " (8.73126127719957, 40.26529873503407),\n",
       " (8.694703548250725, 40.32816927043112),\n",
       " (8.65838451074766, 40.39062931372599),\n",
       " (8.622302606234552, 40.45268154508939),\n",
       " (8.586456286430996, 40.51432862719275),\n",
       " (8.55084401316556, 40.57557320532245),\n",
       " (8.515464258309786, 40.63641790749338),\n",
       " (8.480315503712612, 40.69686534456169),\n",
       " (8.44539624113524, 40.75691811033679),\n",
       " (8.410704972186405, 40.81657878169272),\n",
       " (8.376240208258087, 40.87584991867865),\n",
       " (8.342000470461633, 40.93473406462877),\n",
       " (8.307984289564294, 40.99323374627144),\n",
       " (8.274190205926185, 41.05135147383758),\n",
       " (8.24061676943765, 41.10908974116839),\n",
       " (8.207262539457039, 41.16645102582238),\n",
       " (8.174126084748885, 41.22343778918167),\n",
       " (8.141205983422495, 41.280052476557614),\n",
       " (8.108500822870935, 41.336297517295705),\n",
       " (8.076009199710414, 41.39217532487986),\n",
       " (8.043729719720067, 41.44768829703595),\n",
       " (8.011660997782123, 41.50283881583471),\n",
       " (7.979801657822473, 41.55762924779391),\n",
       " (7.948150332751626, 41.612061943979974),\n",
       " (7.916705664406044, 41.666139240108805),\n",
       " (7.88546630348986, 41.71986345664603),\n",
       " (7.854430909516984, 41.77323689890658),\n",
       " (7.82359815075358, 41.826261857153604),\n",
       " (7.792966704160922, 41.87894060669674),\n",
       " (7.762535255338624, 41.93127540798975),\n",
       " (7.732302498468233, 41.98326850672753),\n",
       " (7.702267136257204, 42.034922133942466),\n",
       " (7.672427879883227, 42.08623850610015),\n",
       " (7.642783448938926, 42.13721982519452),\n",
       " (7.613332571376915, 42.18786827884232),\n",
       " (7.584073983455219, 42.238186040376995),\n",
       " (7.555006429683037, 42.28817526894193),\n",
       " (7.52612866276688, 42.337838109583096),\n",
       " (7.497439443557042, 42.387176693341125),\n",
       " (7.468937540994428, 42.43619313734273),\n",
       " (7.440621732057735, 42.48488954489154),\n",
       " (7.412490801710963, 42.53326800555837),\n",
       " (7.384543542851287, 42.5813305952709),\n",
       " (7.356778756257254, 42.629079376402714),\n",
       " (7.3291952505373255, 42.67651639786184),\n",
       " (7.301791842078755, 42.72364369517862),\n",
       " (7.274567354996798, 42.77046329059311),\n",
       " (7.247520621084258, 42.81697719314181),\n",
       " (7.220650479761354, 42.86318739874389),\n",
       " (7.193955778025923, 42.90909589028684),\n",
       " (7.167435370403942, 42.95470463771156),\n",
       " (7.14108811890038, 43.00001559809685),\n",
       " (7.11491289295036, 43.04503071574346),\n",
       " (7.088908569370651, 43.089751922257435),\n",
       " (7.06307403231147, 43.13418113663308),\n",
       " (7.037408173208604, 43.178320265335266),\n",
       " (7.0119098907358355, 43.22217120238122),\n",
       " (6.986578090757689, 43.265735829421835),\n",
       " (6.961411686282481, 43.30901601582239),\n",
       " (6.936409597415674, 43.35201361874276),\n",
       " (6.911570751313541, 43.39473048321714),\n",
       " (6.886894082137132, 43.43716844223317),\n",
       " (6.862378531006529, 43.47932931681063),\n",
       " (6.838023045955421, 43.521214916079536),\n",
       " (6.813826581885957, 43.56282703735782),\n",
       " (6.789788100523902, 43.60416746622841),\n",
       " (6.765906570374084, 43.64523797661589),\n",
       " (6.742180966676136, 43.686040330862575),\n",
       " (6.718610271360518, 43.72657627980417),\n",
       " (6.695193473004835, 43.76684756284488),\n",
       " (6.671929566790436, 43.806855908032055),\n",
       " (6.6488175544592965, 43.84660303213033),\n",
       " (6.625856444271183, 43.886090640695315),\n",
       " (6.603045250961099, 43.925320428146755),\n",
       " (6.580382995697004, 43.96429407784126),\n",
       " (6.557868706037815, 44.003013262144506),\n",
       " (6.535501415891676, 44.04147964250304),\n",
       " (6.513280165474504, 44.07969486951554),\n",
       " (6.491204001268807, 44.117660583003655),\n",
       " (6.469271975982764, 44.155378412082364),\n",
       " (6.44748314850958, 44.19284997522989),\n",
       " (6.425836583887099, 44.23007688035714),\n",
       " (6.404331353257691, 44.26706072487671),\n",
       " (6.3829665338283865, 44.303803095771414),\n",
       " (6.361741208831288, 44.34030556966242),\n",
       " (6.340654467484224, 44.376569712876844),\n",
       " (6.31970540495167, 44.41259708151502),\n",
       " (6.298893122305925, 44.448389221517225),\n",
       " (6.278216726488532, 44.48394766873005),\n",
       " (6.257675330271961, 44.51927394897229),\n",
       " (6.237268052221538, 44.55436957810041),\n",
       " (6.21699401665762, 44.58923606207361),\n",
       " (6.196852353618022, 44.62387489701841),\n",
       " (6.176842198820685, 44.6582875692929),\n",
       " (6.156962693626591, 44.69247555555048),\n",
       " (6.137212985002917, 44.726440322803235),\n",
       " (6.117592225486433, 44.76018332848489),\n",
       " (6.0980995731471355, 44.79370602051335),\n",
       " (6.078734191552121, 44.827009837352804),\n",
       " (6.059495249729695, 44.8600962080755),\n",
       " (6.040381922133714, 44.892966552423005),\n",
       " (6.021393388608162, 44.925622280867195),\n",
       " (6.002528834351956, 44.95806479467071),\n",
       " (5.983787449883985, 44.990295485947144),\n",
       " (5.9651684310083715, 45.022315737720724),\n",
       " (5.946670978779968, 45.0541269239857),\n",
       " (5.928294299470072, 45.08573040976528),\n",
       " (5.910037604532365, 45.11712755117022),\n",
       " (5.891900110569077, 45.14831969545699),\n",
       " (5.873881039297374, 45.17930818108562),\n",
       " (5.855979617515957, 45.210094337777086),\n",
       " (5.8381950770718865, 45.24067948657042),\n",
       " (5.82052665482762, 45.271064939879366),\n",
       " (5.802973592628267, 45.3012520015487),\n",
       " (5.785535137269054, 45.33124196691018),\n",
       " (5.7682105404630075, 45.36103612283812),\n",
       " (5.750999058808841, 45.390635747804644),\n",
       " (5.7338999537590585, 45.4200421119345),\n",
       " (5.716912491588264, 45.449256477059585),\n",
       " (5.700035943361675, 45.47828009677309),\n",
       " (5.683269584903842, 45.50711421648329),\n",
       " (5.66661269676758, 45.53576007346698),\n",
       " (5.650064564203092, 45.564218896922576),\n",
       " (5.6336244771273, 45.59249190802285),\n",
       " (5.617291730093375, 45.62058031996734),\n",
       " (5.601065622260469, 45.6484853380344),\n",
       " (5.584945457363639, 45.67620815963292),\n",
       " (5.568930543683968, 45.70374997435372),\n",
       " (5.553020194018889, 45.73111196402058),\n",
       " (5.537213725652693, 45.75829530274095),\n",
       " (5.521510460327234, 45.78530115695634),\n",
       " (5.505909724212827, 45.81213068549239),\n",
       " (5.490410847879331, 45.83878503960855),\n",
       " (5.475013166267424, 45.86526536304753),\n",
       " (5.459716018660068, 45.89157279208433),\n",
       " (5.444518748654155, 45.91770845557505),\n",
       " (5.42942070413234, 45.94367347500529),\n",
       " (5.414421237235061, 45.96946896453828),\n",
       " (5.399519704332737, 45.995096031062694),\n",
       " (5.384715465998152, 46.02055577424016),\n",
       " (5.370007886979016, 46.04584928655242),\n",
       " (5.355396336170704, 46.07097765334823),\n",
       " (5.34088018658918, 46.09594195288993),\n",
       " (5.326458815344088, 46.1207432563997),\n",
       " (5.312131603612026, 46.14538262810552),\n",
       " (5.297897936609995, 46.16986112528688),\n",
       " (5.283757203569012, 46.19417979832009),\n",
       " (5.2697087977079065, 46.218339690723404),\n",
       " (5.255752116207284, 46.24234183920175),\n",
       " (5.241886560183656, 46.26618727369125),\n",
       " (5.228111534663744, 46.289877017403406),\n",
       " (5.214426448558947, 46.313412086869),\n",
       " (5.20083071463998, 46.33679349198171),\n",
       " (5.187323749511674, 46.36002223604146),\n",
       " (5.173904973587945, 46.383099315797466),\n",
       " (5.16057381106692, 46.40602572149099),\n",
       " (5.1473296899062335, 46.42880243689786),\n",
       " (5.134172041798478, 46.45143043937066),\n",
       " (5.121100302146819, 46.47391069988068),\n",
       " (5.1081139100407675, 46.49624418305959),\n",
       " (5.095212308232112, 46.51843184724079),\n",
       " (5.082394943111007, 46.5404746445006),\n",
       " (5.069661264682217, 46.56237352069905),\n",
       " (5.057010726541515, 46.58412941552051),\n",
       " (5.0444427858522385, 46.60574326251398),\n",
       " (5.031956903321994, 46.627215989133184),\n",
       " (5.019552543179519, 46.64854851677632),\n",
       " (5.007229173151689, 46.66974176082566),\n",
       " (4.994986264440679, 46.69079663068676),\n",
       " (4.982823291701271, 46.71171402982755),\n",
       " (4.970739733018313, 46.73249485581704),\n",
       " (4.958735069884324, 46.75314000036388),\n",
       " (4.946808787177243, 46.7736503493546),\n",
       " (4.934960373138328, 46.79402678289163),\n",
       " (4.9231893193501906, 46.814270175331075),\n",
       " (4.911495120714987, 46.83438139532022),\n",
       " (4.899877275432738, 46.85436130583481),\n",
       " (4.888335284979802, 46.87421076421606),\n",
       " (4.876868654087478, 46.89393062220749),\n",
       " (4.865476890720758, 46.913521725991444),\n",
       " (4.854159506057209, 46.93298491622538),\n",
       " (4.842916014466003, 46.95232102807798),\n",
       " (4.831745933487076, 46.971530891264976),\n",
       " (4.820648783810423, 46.99061533008473),\n",
       " (4.809624089255536, 47.00957516345365),\n",
       " (4.798671376750964, 47.02841120494128),\n",
       " (4.787790176314023, 47.04712426280525),\n",
       " (4.77698002103062, 47.06571514002593),\n",
       " (4.766240447035221, 47.08418463434091),\n",
       " (4.755570993490949, 47.10253353827921),\n",
       " (4.744971202569804, 47.12076263919531),\n",
       " (4.734440619433023, 47.13887271930292),\n",
       " (4.72397879221156, 47.15686455570852),\n",
       " (4.7135852719866955, 47.17473892044478),\n",
       " (4.7032596127707755, 47.1924965805036),\n",
       " (4.693001371488072, 47.21013829786909),\n",
       " (4.682810107955772, 47.22766482955023),\n",
       " (4.6726853848650896, 47.24507692761336),\n",
       " (4.662626767762498, 47.262375339214465),\n",
       " (4.652633825031091, 47.27956080663121),\n",
       " (4.642706127872061, 47.29663406729482),\n",
       " (4.632843250286296, 47.3135958538217),\n",
       " (4.623044769056104, 47.3304468940449),\n",
       " (4.613310263727053, 47.3471879110453),\n",
       " (4.603639316589923, 47.36381962318269),\n",
       " (4.5940315126627915, 47.380342744126565),\n",
       " (4.584486439673217, 47.39675798288674),\n",
       " (4.575003688040556, 47.4130660438438),\n",
       " (4.565582850858383, 47.42926762677931),\n",
       " (4.5562235238770326, 47.44536342690584),\n",
       " (4.546925305486251, 47.46135413489683),\n",
       " (4.537687796697965, 47.47724043691616),\n",
       " (4.5285106011291605, 47.493023014647655),\n",
       " (4.519393324984872, 47.508702545324304),\n",
       " (4.510335577041288, 47.524279701757344),\n",
       " (4.501336968628962, 47.5397551523651),\n",
       " (4.492397113616133, 47.5551295612017),\n",
       " (4.483515628392159, 47.570403587985545),\n",
       " (4.4746921318510555, 47.58557788812762),\n",
       " (4.46592624537514, 47.60065311275962),\n",
       " (4.45721759281879, 47.61562990876192),\n",
       " (4.448565800492297, 47.63050891879127),\n",
       " (4.439970497145837, 47.64529078130845),\n",
       " (4.431431313953536, 47.65997613060558),\n",
       " (4.422947884497644, 47.6745655968334),\n",
       " (4.414519844752816, 47.68905980602832),\n",
       " (4.406146833070484, 47.7034593801392),\n",
       " (4.397828490163349, 47.71776493705414),\n",
       " (4.389564459089953, 47.73197709062693),\n",
       " (4.38135438523937, 47.7460964507034),\n",
       " (4.373197916315988, 47.7601236231476),\n",
       " (4.365094702324388, 47.7740592098678),\n",
       " (4.3570443955543325, 47.787903808842316),\n",
       " (4.34904665056584, 47.80165801414515),\n",
       " (4.3411011241743624, 47.815322415971494),\n",
       " (4.333207475436063, 47.828897600663076),\n",
       " (4.325365365633181, 47.842384150733295),\n",
       " (4.317574458259501, 47.85578264489222),\n",
       " (4.309834419005912, 47.869093658071435),\n",
       " (4.302144915746063, 47.88231776144869),\n",
       " (4.29450561852211, 47.895455522472425),\n",
       " (4.286916199530557, 47.90850750488612),\n",
       " (4.279376333108194, 47.92147426875247),\n",
       " (4.271885695718119, 47.93435637047744),\n",
       " (4.264443965935853, 47.94715436283413),\n",
       " (4.257050824435554, 47.95986879498648),\n",
       " (4.24970595397631, 47.972500212512855),\n",
       " (4.2424090393885265, 47.98504915742945),\n",
       " (4.235159767560404, 47.99751616821355),\n",
       " (4.227957827424501, 48.00990177982663),\n",
       " (4.220802909944388, 48.0222065237373),\n",
       " (4.213694708101384, 48.03443092794415),\n",
       " (4.206632916881384, 48.04657551699836),\n",
       " (4.199617233261772, 48.05864081202623),\n",
       " (4.192647356198413, 48.070627330751556),\n",
       " (4.185722986612743, 48.0825355875178),\n",
       " (4.178843827378928, 48.09436609331023),\n",
       " (4.17200958331112, 48.10611935577777),\n",
       " (4.165219961150788, 48.11779587925486),\n",
       " (4.158474669554131, 48.12939616478301),\n",
       " (4.151773419079584, 48.140920710132384),\n",
       " (4.145115922175392, 48.1523700098231),\n",
       " (4.1385018931672715, 48.163744555146494),\n",
       " (4.131931048246156, 48.17504483418616),\n",
       " (4.125403105456014, 48.186271331838924),\n",
       " (4.118917784681752, 48.19742452983564),\n",
       " (4.112474807637193, 48.20850490676185),\n",
       " (4.106073897853138, 48.21951293807836),\n",
       " (4.0997147806655, 48.230449096141584),\n",
       " (4.0933971832035185, 48.24131385022386),\n",
       " (4.0871208343780525, 48.25210766653356),\n",
       " (4.080885464869946, 48.26283100823512),\n",
       " (4.074690807118474, 48.27348433546887),\n",
       " (4.0685365953098565, 48.28406810537084),\n",
       " (4.062422565365857, 48.294582772092326),\n",
       " (4.056348454932451, 48.30502878681939),\n",
       " (4.050314003368564, 48.31540659779225),\n",
       " (4.044318951734893, 48.32571665032446),\n",
       " (4.03836304278279, 48.33595938682206),\n",
       " (4.032446020943227, 48.34613524680256),\n",
       " (4.026567632315827, 48.35624466691378),\n",
       " (4.020727624657973, 48.366288080952586),\n",
       " (4.01492574737398, 48.37626591988352),\n",
       " (4.009161751504344, 48.38617861185728),\n",
       " (4.003435389715057, 48.39602658222911),\n",
       " (3.997746416286999, 48.40581025357703),\n",
       " (3.9920945871053872, 48.41553004571997),\n",
       " (3.9864796596493077, 48.425186375735805),\n",
       " (3.9809013929813033, 48.43477965797924),\n",
       " (3.975359547737038, 48.444310304099595),\n",
       " (3.9698538861150245, 48.45377872305844),\n",
       " (3.964384171866422, 48.46318532114719),\n",
       " (3.958950170284896, 48.47253050200452),\n",
       " (3.9535516481965494, 48.481814666633646),\n",
       " (3.948188373949915, 48.491038213419614),\n",
       " (3.9428601174060174, 48.500201538146314),\n",
       " (3.937566649928496, 48.509305034013515),\n",
       " (3.932307744373794, 48.51834909165372),\n",
       " (3.9270831750814135, 48.52733409914891),\n",
       " (3.921892717864231, 48.53626044204724),\n",
       " (3.916736149998877, 48.54512850337954),\n",
       " (3.9116132502161802, 48.55393866367577),\n",
       " (3.9065237986916728, 48.56269130098136),\n",
       " (3.9014675770361555, 48.57138679087341),\n",
       " (3.89644436828633, 48.58002550647681),\n",
       " (3.8914539568954862, 48.588607818480256),\n",
       " (3.886496128724254, 48.597134095152164),\n",
       " (3.8815706710314157, 48.605604702356466),\n",
       " (3.876677372464775, 48.6140200035683),\n",
       " (3.871816023052089, 48.62238035988962),\n",
       " (3.8669864141920587, 48.63068613006469),\n",
       " (3.8621883386453773, 48.63893767049546),\n",
       " (3.857421590525837, 48.647135335256884),\n",
       " (3.852685965291496, 48.6552794761121),\n",
       " (3.8479812597359, 48.66337044252752),\n",
       " (3.8433072719793637, 48.67140858168785),\n",
       " (3.838663801460307, 48.679394238510945),\n",
       " (3.8340506489266497, 48.68732775566265),\n",
       " (3.8294676164272614, 48.69520947357149),\n",
       " (3.824914507303467, 48.703039730443265),\n",
       " (3.8203911261806085, 48.71081886227558),\n",
       " (3.8158972789596617, 48.71854720287226),\n",
       " (3.811432772808906, 48.726225083857656),\n",
       " (3.8069974161556526, 48.73385283469091),\n",
       " (3.8025910186780205, 48.74143078268005),\n",
       " (3.7982133912967724, 48.748959252996066),\n",
       " (3.7938643461672013, 48.75643856868686),\n",
       " (3.789543696671068, 48.763869050691085),\n",
       " (3.7852512574085955, 48.771251017851945),\n",
       " (3.780986844190512, 48.77858478693086),\n",
       " (3.7767502740301486, 48.78587067262105),\n",
       " (3.7725413651355857, 48.79310898756108),\n",
       " (3.7683599369018537, 48.80030004234821),\n",
       " (3.7642058099031828, 48.8074441455518),\n",
       " (3.760078805885303, 48.81454160372647),\n",
       " (3.7559787477577964, 48.82159272142532),\n",
       " (3.751905459586497, 48.828597801212965),\n",
       " (3.747858766585943, 48.83555714367853),\n",
       " (3.743838495111875, 48.84247104744853),\n",
       " (3.7398444726537856, 48.84933980919971),\n",
       " (3.7358765278275174, 48.85616372367177),\n",
       " (3.731934490367909, 48.86294308367999),\n",
       " (3.7280181911214862, 48.86967818012781),\n",
       " (3.724127462039208, 48.87636930201934),\n",
       " (3.7202621361692523, 48.8830167364717),\n",
       " (3.716422047649853, 48.8896207687274),\n",
       " (3.7126070317021838, 48.896181682166535),\n",
       " (3.708816924623285, 48.90269975831899),\n",
       " (3.7050515637790418, 48.90917527687647),\n",
       " (3.7013107875972033, 48.915608515704534),\n",
       " (3.6975944355604518, 48.92199975085452),\n",
       " (3.6939023481995124, 48.928349256575366),\n",
       " (3.6902343670863123, 48.93465730532541),\n",
       " (3.686590334827182, 48.940924167784054),\n",
       " (3.6829700950560995, 48.9471501128634),\n",
       " (3.679373492427984, 48.953335407719756),\n",
       " (3.675800372612028, 48.95948031776515),\n",
       " (3.672250582285075, 48.96558510667867),\n",
       " (3.6687239691250406, 48.971650036417806),\n",
       " (3.6652203818043776, 48.97767536722969),\n",
       " (3.66173966998358, 48.983661357662264),\n",
       " (3.658281684304733, 48.98960826457534),\n",
       " (3.654846276385106, 48.99551634315167),\n",
       " (3.6514332988107823, 49.00138584690789),\n",
       " (3.6480426051303354, 49.007217027705344),\n",
       " (3.6446740498485446, 49.01301013576097),\n",
       " (3.6413274884201505, 49.01876541965798),\n",
       " (3.638002777243655, 49.02448312635655),\n",
       " (3.6346997736551567, 49.030163501204406),\n",
       " (3.631418335922231, 49.035806787947365),\n",
       " (3.6281583232378476, 49.04141322873978),\n",
       " (3.6249195957143288, 49.046983064154944),\n",
       " (3.621702014377346, 49.052516533195416),\n",
       " (3.6185054411599573, 49.05801387330325),\n",
       " (3.6153297388966825, 49.063475320370216),\n",
       " (3.6121747713176178, 49.068901108747895),\n",
       " (3.609040403042588, 49.07429147125776),\n",
       " (3.605926499575337, 49.079646639201144),\n",
       " (3.6028329272977566, 49.084966842369184),\n",
       " (3.599759553464155, 49.09025230905267),\n",
       " (3.596706246195557, 49.09550326605184),\n",
       " (3.593672874474048, 49.100719938686126),\n",
       " (3.590659308137151, 49.1059025508038),\n",
       " (3.5876654178722407, 49.11105132479159),\n",
       " (3.5846910752109955, 49.11616648158423),\n",
       " (3.5817361525238853, 49.12124824067394),\n",
       " (3.578800523014694, 49.12629682011984),\n",
       " (3.5758840607150786, 49.131312436557295),\n",
       " (3.5729866404791646, 49.136295305207234),\n",
       " (3.5701081379781745, 49.14124563988536),\n",
       " (3.567248429695095, 49.14616365301135),\n",
       " (3.564407392919375, 49.151049555617945),\n",
       " (3.5615849057416615, 49.15590355736003),\n",
       " (3.5587808470485682, 49.160725866523606),\n",
       " (3.5559950965174774, 49.16551669003476),\n",
       " (3.5532275346113775, 49.170276233468506),\n",
       " (3.5504780425737343, 49.17500470105764),\n",
       " (3.547746502423394, 49.179702295701475),\n",
       " (3.545032796949522, 49.18436921897457),\n",
       " (3.5423368097065713, 49.189005671135355),\n",
       " (3.539658425009289, 49.19361185113477),\n",
       " (3.536997527927748, 49.198187956624736),\n",
       " (3.5343540042824197, 49.2027341839667),\n",
       " (3.5317277406392713, 49.20725072824001),\n",
       " (3.5291186243049, 49.211737783250335),\n",
       " (3.526526543321697, 49.21619554153793),\n",
       " (3.5239513864630427, 49.220624194385934),\n",
       " (3.5213930432285347, 49.225023931828574),\n",
       " (3.5188514038392458, 49.22939494265929),\n",
       " (3.5163263592330143, 49.233737414438885),\n",
       " (3.513817801059762, 49.23805153350353),\n",
       " (3.511325621676847, 49.24233748497278),\n",
       " (3.508849714144444, 49.24659545275751),\n",
       " (3.5063899722209557, 49.25082561956782),\n",
       " (3.503946290358453, 49.25502816692086),\n",
       " (3.501518563698147, 49.25920327514862),\n",
       " (3.4991066880658903, 49.263351123405684),\n",
       " (3.4967105599677035, 49.26747188967689),\n",
       " (3.4943300765853387, 49.27156575078501),\n",
       " (3.4919651357718635, 49.27563288239828),\n",
       " (3.4896156360472808, 49.27967345903799),\n",
       " (3.4872814765941724, 49.28368765408594),\n",
       " (3.484962557253374, 49.2876756397919),\n",
       " (3.4826587785196774, 49.29163758728099),\n",
       " (3.48037004153756, 49.295573666561026),\n",
       " (3.4780962480969424, 49.29948404652981),\n",
       " (3.4758373006289767, 49.3033688949824),\n",
       " (3.4735931022018565, 49.30722837861826),\n",
       " (3.47136355651666, 49.31106266304848),\n",
       " (3.469148567903217, 49.31487191280282),\n",
       " (3.4669480413160034, 49.31865629133682),\n",
       " (3.4647618823300634, 49.322415961038786),\n",
       " (3.4625899971369565, 49.326151083236745),\n",
       " (3.460432292540734, 49.329861818205416),\n",
       " (3.4582886759539373, 49.33354832517304),\n",
       " (3.4561590553936288, 49.33721076232823),\n",
       " (3.4540433394774412, 49.340849286826774),\n",
       " (3.451941437419658, 49.344464054798344),\n",
       " (3.4498532590273183, 49.34805522135323),\n",
       " (3.447778714696345, 49.351622940588975),\n",
       " (3.4457177154077008, 49.355167365597),\n",
       " (3.4436701727235692, 49.358688648469155),\n",
       " (3.4416359987835587, 49.36218694030427),\n",
       " (3.4396151063009324, 49.36566239121461),\n",
       " (3.4376074085588626, 49.36911515033236),\n",
       " (3.4356128194067113, 49.372545365815974),\n",
       " (3.4336312532563302, 49.37595318485655),\n",
       " (3.4316626250783924, 49.379338753684166),\n",
       " (3.4297068503987402, 49.38270221757414),\n",
       " (3.427763845294762, 49.38604372085325),\n",
       " (3.42583352639179, 49.38936340690596),\n",
       " (3.423915810859525, 49.39266141818053),\n",
       " (3.4220106164084787, 49.39593789619517),\n",
       " (3.420117861286446, 49.39919298154409),\n",
       " (3.418237464274994, 49.40242681390353),\n",
       " (3.4163693446859793, 49.40563953203777),\n",
       " (3.414513422358085, 49.40883127380506),\n",
       " (3.4126696176533797, 49.41200217616358),\n",
       " (3.410837851453902, 49.415152375177264),\n",
       " (3.4090180451582635, 49.418282006021656),\n",
       " (3.407210120678279, 49.42139120298974),\n",
       " (3.405414000435613, 49.42448009949766),\n",
       " (3.403629607358451, 49.42754882809047),\n",
       " (3.401856864878195, 49.430597520447826),\n",
       " (3.400095696926174, 49.43362630738961),\n",
       " (3.3983460279303834, 49.436635318881564),\n",
       " (3.3966077828122403, 49.439624684040865),\n",
       " (3.3948808869833615, 49.442594531141665),\n",
       " (3.393165266342365, 49.445544987620586),\n",
       " (3.391460847271687, 49.448476180082196),\n",
       " (3.389767556634427, 49.451388234304446),\n",
       " (3.388085321771207, 49.454281275244064),\n",
       " (3.3864140704970525, 49.457155427041904),\n",
       " (3.3847537310982982, 49.46001081302829),\n",
       " (3.383104232329509, 49.4628475557283),\n",
       " (3.381465503410422, 49.46566577686704),\n",
       " (3.3798374740229113, 49.46846559737483),\n",
       " (3.37822007430797, 49.47124713739243),\n",
       " (3.37661323486271, 49.474010516276174),\n",
       " (3.3750168867373893, 49.476755852603105),\n",
       " (3.3734309614324474, 49.479483264176054),\n",
       " (3.37185539089557, 49.4821928680287),\n",
       " (3.3702901075187683, 49.48488478043058),\n",
       " (3.3687350441354753, 49.487559116892115),\n",
       " (3.3671901340176666, 49.4902159921695),\n",
       " (3.365655310872996, 49.4928555202697),\n",
       " (3.3641305088419515, 49.49547781445529),\n",
       " (3.362615662495028, 49.49808298724935),\n",
       " (3.3611107068299213, 49.50067115044026),\n",
       " (3.359615577268737, 49.50324241508654),\n",
       " (3.3581302096552212, 49.50579689152157),\n",
       " (3.356654540252006, 49.50833468935835),\n",
       " (3.3551885057378756, 49.510855917494204),\n",
       " (3.3537320432050493, 49.51336068411545),\n",
       " (3.3522850901564816, 49.51584909670202),\n",
       " (3.3508475845031795, 49.51832126203212),\n",
       " (3.349419464561541, 49.52077728618675),\n",
       " (3.348000669050705, 49.52321727455433),\n",
       " (3.346591137089924, 49.525641331835146),\n",
       " (3.3451908081959507, 49.5280495620459),\n",
       " (3.343799622280443, 49.53044206852414),\n",
       " (3.3424175196473858, 49.53281895393271),\n",
       " (3.3410444409905287, 49.53518032026416),\n",
       " (3.3396803273908415, 49.5375262688451),\n",
       " (3.3383251203139865, 49.53985690034057),\n",
       " (3.3369787616078064, 49.54217231475834),\n",
       " (3.3356411934998285, 49.54447261145324),\n",
       " (3.334312358594786, 49.54675788913136),\n",
       " (3.332992199872156, 49.549028245854345),\n",
       " (3.3316806606837117, 49.55128377904358),\n",
       " (3.3303776847510913, 49.55352458548436),\n",
       " (3.329083216163384, 49.55575076133005),\n",
       " (3.327797199374731, 49.55796240210623),\n",
       " (3.3265195792019413, 49.560159602714755),\n",
       " (3.3252503008221246, 49.56234245743788),\n",
       " (3.323989309770337, 49.56451105994224),\n",
       " (3.322736551937247, 49.56666550328294),\n",
       " (3.32149197356681, 49.56880587990749),\n",
       " (3.3202555212539644, 49.57093228165979),\n",
       " (3.31902714194234, 49.57304479978411),\n",
       " (3.3178067829219784, 49.575143524928926),\n",
       " (3.3165943918270746, 49.577228547150895),\n",
       " (3.315389916633728, 49.57929995591865),\n",
       " (3.3141933056577115, 49.58135784011669),\n",
       " (3.3130045075522516, 49.58340228804915),\n",
       " (3.3118234713058277, 49.585433387443636),\n",
       " (3.31065014623998, 49.587451225454956),\n",
       " (3.3094844820071385, 49.589455888668866),\n",
       " (3.3083264285884604, 49.591447463105794),\n",
       " (3.3071759362916833, 49.59342603422453),\n",
       " (3.3060329557489943, 49.59539168692587),\n",
       " (3.3048974379149114, 49.59734450555631),\n",
       " (3.3037693340641785, 49.5992845739116),\n",
       " (3.3026485957896745, 49.60121197524041),\n",
       " (3.3015351750003368, 49.603126792247835),\n",
       " (3.3004290239190963, 49.605029107098986),\n",
       " (3.2993300950808298, 49.606919001422504),\n",
       " (3.298238341330321, 49.60879655631407),\n",
       " (3.2971537158202375, 49.61066185233987),\n",
       " (3.296076172009121, 49.61251496954005),\n",
       " (3.2950056636593903, 49.61435598743219),\n",
       " (3.293942144835357, 49.61618498501466),\n",
       " (3.292885569901254, 49.61800204077005),\n",
       " (3.2918358935192775, 49.61980723266852),\n",
       " (3.2907930706476427, 49.621600638171145),\n",
       " (3.289757056538649, 49.62338233423325),\n",
       " (3.288727806736762, 49.6251523973077),\n",
       " (3.287705277076704, 49.626910903348204),\n",
       " (3.28668942368156, 49.62865792781254),\n",
       " (3.2856802029608936, 49.63039354566581),\n",
       " (3.2846775716088787, 49.63211783138366),\n",
       " (3.2836814866024406, 49.63383085895547),\n",
       " (3.282691905199408, 49.63553270188754),\n",
       " (3.2817087849366815, 49.63722343320623),\n",
       " (3.2807320836284086, 49.63890312546109),\n",
       " (3.279761759364177, 49.640571850727994),\n",
       " (3.2787977705072135, 49.64222968061222),\n",
       " (3.2778400756925987, 49.64387668625151),\n",
       " (3.276888633825491, 49.64551293831915),\n",
       " (3.2759434040793636, 49.64713850702699),\n",
       " (3.2750043458942546, 49.64875346212844),\n",
       " (3.274071418975023, 49.6503578729215),\n",
       " (3.2731445832896213, 49.65195180825169),\n",
       " (3.272223799067379, 49.653535336515034),\n",
       " (3.271309026797294, 49.655108525661),\n",
       " (3.270400227226339, 49.656671443195386),\n",
       " (3.269497361357775, 49.65822415618324),\n",
       " (3.2686003904494805, 49.65976673125172),\n",
       " (3.2677092760122863, 49.66129923459299),\n",
       " (3.2668239798083265, 49.66282173196701),\n",
       " (3.265944463849396, 49.664334288704396),\n",
       " (3.2650706903953206, 49.665836969709204),\n",
       " (3.264202621952339, 49.66732983946172),\n",
       " (3.2633402212714917, 49.668812962021235),\n",
       " (3.262483451347024, 49.670286401028775),\n",
       " (3.261632275414799, 49.67175021970986),\n",
       " (3.2607866569507173, 49.67320448087717),\n",
       " (3.2599465596691526, 49.6746492469333),\n",
       " (3.259111947521393, 49.676084579873404),\n",
       " (3.2582827846940945, 49.677510541287845),\n",
       " (3.257459035607744, 49.67892719236487),\n",
       " (3.256640664915134, 49.68033459389319),\n",
       " (3.255827637499844, 49.681732806264655),\n",
       " (3.2550199184747344, 49.68312188947678),\n",
       " (3.254217473180449, 49.68450190313535),\n",
       " (3.2534202671839303, 49.68587290645698),\n",
       " (3.252628266276937, 49.687234958271645),\n",
       " (3.251841436474581, 49.68858811702521),\n",
       " (3.251059744013867, 49.689932440781945),\n",
       " (3.2502831553522435, 49.691267987227),\n",
       " (3.2495116371661634, 49.69259481366889),\n",
       " (3.248745156349655, 49.693912977041954),\n",
       " (3.247983680012901, 49.6952225339088),\n",
       " (3.2472271754808277, 49.69652354046272),\n",
       " (3.2464756102917014, 49.697816052530136),\n",
       " (3.2457289521957366, 49.69910012557294),\n",
       " (3.2449871691537124, 49.700375814690915),\n",
       " (3.244250229335597, 49.701643174624095),\n",
       " (3.2435181011191814, 49.7029022597551),\n",
       " (3.242790753088725, 49.70415312411148),\n",
       " (3.2420681540336043, 49.70539582136802),\n",
       " (3.241350272946976, 49.70663040484908),\n",
       " (3.2406370790244448, 49.70785692753083),\n",
       " (3.2399285416627426, 49.709075442043556),\n",
       " (3.2392246304584154, 49.71028600067392),\n",
       " (3.2385253152065183, 49.71148865536719),\n",
       " (3.237830565899319, 49.712683457729476),\n",
       " (3.237140352725011, 49.71387045902994),\n",
       " (3.2364546460664343, 49.71504971020301),\n",
       " (3.2357734164998035, 49.71622126185054),\n",
       " (3.2350966347934467, 49.71738516424401),\n",
       " (3.23442427190655, 49.71854146732665),\n",
       " (3.233756298987912, 49.71969022071563),\n",
       " (3.233092687374705, 49.72083147370413),\n",
       " (3.2324334085912456, 49.721965275263514),\n",
       " (3.231778434347774, 49.723091674045385),\n",
       " (3.2311277365392383, 49.7242107183837),\n",
       " (3.230481287244089, 49.725322456296844),\n",
       " (3.229839058723081, 49.72642693548966),\n",
       " (3.2292010234180832, 49.72752420335554),\n",
       " (3.228567153950896, 49.72861430697843),\n",
       " (3.227937423122077, 49.72969729313485),\n",
       " (3.2273118039097732, 49.73077320829591),\n",
       " (3.2266902694685617, 49.73184209862931),\n",
       " (3.226072793128298, 49.732904010001306),\n",
       " (3.225459348392971, 49.733958987978696),\n",
       " (3.224849908939567, 49.73500707783075),\n",
       " (3.224244448616939, 49.73604832453118),\n",
       " (3.223642941444685, 49.737082772760054),\n",
       " (3.2230453616120345, 49.73811046690571),\n",
       " (3.2224516834767387, 49.73913145106667),\n",
       " (3.2218618815639717, 49.740145769053534),\n",
       " (3.2212759305652368, 49.741153464390855),\n",
       " (3.220693805337281, 49.742154580319),\n",
       " (3.2201154809010153, 49.743149159796026),\n",
       " (3.2195409324404434, 49.744137245499495),\n",
       " (3.218970135301597, 49.74511887982833),\n",
       " (3.218403064991477, 49.74609410490462),\n",
       " (3.217839697177004, 49.747062962575434),\n",
       " (3.217280007683972, 49.748025494414605),\n",
       " (3.2167239724960135, 49.748981741724535),\n",
       " (3.2161715677535683, 49.74993174553796),\n",
       " (3.215622769752858, 49.75087554661969),\n",
       " (3.215077554944871, 49.751813185468386),\n",
       " (3.21453589993435, 49.75274470231829),\n",
       " (3.21399778147879, 49.753670137140936),\n",
       " (3.21346317648744, 49.75458952964689),\n",
       " (3.2129320620203123, 49.75550291928744),\n",
       " (3.212404415287198, 49.75641034525626),\n",
       " (3.211880213646689, 49.75731184649118),\n",
       " (3.211359434605208, 49.75820746167574),\n",
       " (3.2108420558160407, 49.75909722924094),\n",
       " (3.2103280550783784, 49.75998118736686),\n",
       " (3.2098174103363655, 49.76085937398428),\n",
       " (3.2093100996781527, 49.76173182677634),\n",
       " (3.208806101334956, 49.76259858318013),\n",
       " (3.2083053936801242, 49.7634596803883),\n",
       " (3.2078079552282097, 49.76431515535068),\n",
       " (3.2073137646340477, 49.765165044775834),\n",
       " (3.2068228006918384, 49.76600938513267),\n",
       " (3.206335042334239, 49.76684821265196),\n",
       " (3.205850468631458, 49.767681563327926),\n",
       " (3.205369058790359, 49.768509472919796),\n",
       " (3.2048907921535665, 49.769331976953296),\n",
       " (3.204415648198581, 49.77014911072221),\n",
       " (3.203943606536898, 49.77096090928988),\n",
       " (3.203474646913132, 49.77176740749073),\n",
       " (3.2030087492041486, 49.772568639931706),\n",
       " (3.2025458934182014, 49.77336464099384),\n",
       " (3.2020860596940723, 49.77415544483367),\n",
       " (3.2016292283002215, 49.774941085384704),\n",
       " (3.2011753796339386, 49.77572159635891),\n",
       " (3.2007244942205033, 49.77649701124815),\n",
       " (3.200276552712349, 49.777267363325606),\n",
       " (3.1998315358882317, 49.778032685647204),\n",
       " (3.199389424652406, 49.77879301105305),\n",
       " (3.1989502000338077, 49.77954837216883),\n",
       " (3.198513843185236, 49.78029880140722),\n",
       " (3.198080335382547, 49.78104433096926),\n",
       " (3.19764965802385, 49.78178499284574),\n",
       " (3.1972217926287088, 49.78252081881859),\n",
       " (3.1967967208373493, 49.78325184046221),\n",
       " (3.1963744244098717, 49.78397808914488),\n",
       " (3.1959548852254667, 49.78469959603003),\n",
       " (3.1955380852816395, 49.78541639207765),\n",
       " (3.1951240066934363, 49.78612850804557),\n",
       " (3.194712631692677, 49.786835974490806),\n",
       " (3.1943039426271924, 49.78753882177085),\n",
       " (3.193897921960069, 49.78823708004501),\n",
       " (3.1934945522688927, 49.788930779275645),\n",
       " (3.193093816245004, 49.78961994922952),\n",
       " (3.192695696692755, 49.790304619479016),\n",
       " (3.19230017652877, 49.790984819403455),\n",
       " (3.1919072387812144, 49.791660578190324),\n",
       " (3.191516866589065, 49.792331924836546),\n",
       " (3.191129043201387, 49.79299888814971),\n",
       " (3.190743751976615, 49.79366149674932),\n",
       " (3.1903609763818404, 49.79431977906803),\n",
       " (3.1899806999920988, 49.794973763352836),\n",
       " (3.189602906489668, 49.795623477666304),\n",
       " (3.189227579663368, 49.79626894988779),\n",
       " (3.188854703407862, 49.79691020771461),\n",
       " (3.1884842617229694, 49.797547278663245),\n",
       " (3.188116238712977, 49.79818019007051),\n",
       " (3.187750618585957, 49.798808969094736),\n",
       " (3.18738738565309, 49.799433642716934),\n",
       " (3.1870265243279916, 49.80005423774195),\n",
       " (3.186668019126043, 49.80067078079963),\n",
       " (3.1863118546637277, 49.80128329834592),\n",
       " (3.18595801565797, 49.80189181666407),\n",
       " (3.1856064869254803, 49.80249636186568),\n",
       " (3.185257253382103, 49.803096959891896),\n",
       " (3.184910300042169, 49.803693636514474),\n",
       " (3.1845656120178543, 49.804286417336904),\n",
       " (3.184223174518539, 49.80487532779551),\n",
       " (3.1838829728501747, 49.80546039316053),\n",
       " (3.1835449924146513, 49.80604163853722),\n",
       " (3.183209218709173, 49.80661908886692),\n",
       " (3.1828756373256364, 49.807192768928104),\n",
       " (3.1825442339500083, 49.80776270333749),\n",
       " (3.182214994361716, 49.80832891655105),\n",
       " (3.1818879044330357, 49.80889143286509),\n",
       " (3.181562950128484, 49.80945027641727),\n",
       " (3.1812401175042186, 49.81000547118767),\n",
       " (3.1809193927074393, 49.81055704099978),\n",
       " (3.180600761975792, 49.811105009521555),\n",
       " (3.18028421163678, 49.81164940026641),\n",
       " (3.179969728107176, 49.81219023659425),\n",
       " (3.1796572978924407, 49.812727541712434),\n",
       " (3.1793469075861425, 49.813261338676824),\n",
       " (3.1790385438693822, 49.81379165039274),\n",
       " (3.1787321935102226, 49.81431849961593),\n",
       " (3.1784278433631195, 49.81484190895359),\n",
       " (3.178125480368358, 49.8153619008653),\n",
       " (3.177825091551493, 49.815878497663995),\n",
       " (3.177526664022791, 49.81639172151693),\n",
       " (3.1772301849766773, 49.81690159444663),\n",
       " (3.176935641691188, 49.817408138331814),\n",
       " (3.1766430215274224, 49.81791137490837),\n",
       " (3.1763523119290014, 49.81841132577026),\n",
       " (3.1760635004215283, 49.818908012370464),\n",
       " (3.1757765746120548, 49.81940145602188),\n",
       " (3.1754915221885462, 49.81989167789825),\n",
       " (3.1752083309193577, 49.82037869903508),\n",
       " (3.1749269886527047, 49.82086254033053),\n",
       " (3.1746474833161447, 49.821343222546304),\n",
       " (3.174369802916058, 49.821820766308555),\n",
       " (3.1740939355371336, 49.82229519210877),\n",
       " (3.1738198693418567, 49.82276652030463),\n",
       " (3.173547592570002, 49.823234771120916),\n",
       " (3.1732770935381285, 49.82369996465034),\n",
       " (3.1730083606390784, 49.824162120854425),\n",
       " (3.1727413823414796, 49.82462125956438),\n",
       " (3.172476147189249, 49.82507740048192),\n",
       " (3.1722126438011027, 49.82553056318013),\n",
       " (3.1719508608700675, 49.82598076710429),\n",
       " (3.1716907871629947, 49.826428031572725),\n",
       " (3.1714324115200796, 49.826872375777626),\n",
       " (3.17117572285438, 49.827313818785875),\n",
       " (3.1709207101513446, 49.82775237953987),\n",
       " (3.1706673624683357, 49.82818807685831),\n",
       " (3.170415668934163, 49.82862092943706),\n",
       " (3.1701656187486154, 49.82905095584987),\n",
       " (3.169917201181999, 49.829478174549266),\n",
       " (3.1696704055746747, 49.82990260386726),\n",
       " (3.169425221336603, 49.83032426201619),\n",
       " (3.1691816379468882, 49.83074316708947),\n",
       " (3.168939644953327, 49.831159337062395),\n",
       " (3.1686992319719605, 49.83157278979288),\n",
       " (3.168460388686628, 49.83198354302225),\n",
       " (3.1682231048485248, 49.832391614375986),\n",
       " (3.167987370275763, 49.832797021364506),\n",
       " (3.1677531748529337, 49.83319978138389),\n",
       " (3.167520508530673, 49.83359991171663),\n",
       " (3.1672893613252304, 49.83399742953239),\n",
       " (3.167059723318042, 49.834392351888724),\n",
       " (3.166831584655303, 49.83478469573181),\n",
       " (3.1666049355475456, 49.8351744778972),\n",
       " (3.16637976626922, 49.83556171511049),\n",
       " (3.1661560671582762, 49.835946423988105),\n",
       " (3.165933828615748, 49.836328621037964),\n",
       " (3.165713041105343, 49.83670832266021),\n",
       " (3.1654936951530335, 49.837085545147886),\n",
       " (3.1652757813466486, 49.83746030468769),\n",
       " (3.165059290335471, 49.837832617360604),\n",
       " (3.1648442128298364, 49.838202499142625),\n",
       " (3.1646305396007333, 49.83856996590545),\n",
       " (3.1644182614794083, 49.83893503341713),\n",
       " (3.164207369356973, 49.839297717342774),\n",
       " (3.163997854184012, 49.83965803324521),\n",
       " (3.163789706970195, 49.84001599658565),\n",
       " (3.1635829187838898, 49.840371622724355),\n",
       " (3.1633774807517825, 49.84072492692131),\n",
       " (3.1631733840584935, 49.841075924336856),\n",
       " (3.1629706199461998, 49.84142463003234),\n",
       " (3.1627691797142603, 49.84177105897079),\n",
       " (3.162569054718842, 49.842115226017526),\n",
       " (3.162370236372548, 49.842457145940806),\n",
       " (3.162172716144051, 49.84279683341248),\n",
       " (3.1619764855577257, 49.84313430300858)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our  m  and  b  values both update with each step. Not only that, but with each step, the size of the changes to  m and  b  decrease. This is because they are approaching a best fit line.\n",
    "\n",
    "## Let's include 2 predictors, $x_1$ and $x_2$\n",
    "\n",
    "Below, we generated a problem where we have 2 predictors. We generated data such that the best fit line is around $\\hat y = 3x_1 -4x_2 +2$, noting that there is random noise introduced, so the final result will never be exactly that. Let's build what we built previously, but now create a `step_gradient_multi` function that can take an *arbitrary* number of predictors (so the function should be able to include more than 2 predictors as well). Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(11)\n",
    "\n",
    "x1 = np.random.rand(100,1).reshape(100)\n",
    "x2 = np.random.rand(100,1).reshape(100)\n",
    "y_randterm = np.random.normal(0,0.2,100)\n",
    "y = 2+ 3* x1+ -4*x2 + y_randterm\n",
    "\n",
    "data = np.array([y, x1, x2])\n",
    "data = np.transpose(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5), sharey=True)\n",
    "ax1.set_title('x_1')\n",
    "ax1.plot(x1, y, '.b')\n",
    "ax2.set_title('x_2')\n",
    "ax2.plot(x2, y, '.b');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, for our gradients, when having multiple predictors $x_j$ with $j \\in 1,\\ldots, k$\n",
    "\n",
    "$$ \\frac{dJ}{dm_j}J(m_j,b) = -2\\sum_{i = 1}^n x_{j,i}(y_i - (\\sum_{j=1}^km{x_{j,i}} + b)) = -2\\sum_{i = 1}^n x_{j,i}*\\epsilon_i$$\n",
    "$$ \\frac{dJ}{db}J(m_j,b) = -2\\sum_{i = 1}^n(y_i - (\\sum_{j=1}^km{x_{j,i}} + b)) = -2\\sum_{i = 1}^n \\epsilon_i $$\n",
    "    \n",
    "\n",
    "So we'll have one gradient per predictor along with the gradient for the intercept!\n",
    "\n",
    "Create the `step_gradient_multi` function below. As we said before, this means that we have more than one feature that we are using as an independent variable in the regression. This function will have the same inputs as `step_gradient`, but it will be able to handle having more than one value for m. It should return the final values for b and m in the form of a tuple.\n",
    "\n",
    "- `b_current` refers to the y-intercept at the current step\n",
    "- `m_current` refers to the slope at the current step\n",
    "- `points` are the data points to which we want to fit a line\n",
    "\n",
    "You might have to refactor your `error` at function if you want to use it with multiple m values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient_multi(b_current, m_current ,points):\n",
    "    b_gradient = 0\n",
    "    m_gradient = np.zeros(len(m_current))\n",
    "    learning_rate = .1\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        y = points[i][0]\n",
    "        x = points[i][1:(len(m_current)+1)] \n",
    "        b_gradient += -(1/N)  * (y -  (sum(m_current * x) + b_current))\n",
    "        m_gradient += -(1/N) * x * (y -  (sum(m_current * x) + b_current))\n",
    "    new_b = b_current - (learning_rate * b_gradient)\n",
    "    new_m = m_current - (learning_rate * m_gradient)\n",
    "    return (new_b, new_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply 1 step to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.0250308395837813, 2.0728619246505193)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 0\n",
    "m = [0,0]\n",
    "updated_b, updated_m = step_gradient_multi(b, m, data) # {'b': 0.0085, 'm': 0.6249999999999999}\n",
    "(update_to_b, update_to_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply 500 steps to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set our initial step with m and b values, and the corresponding error.\n",
    "b = 0\n",
    "m = [0,0]\n",
    "iterations = []\n",
    "for i in range(500):\n",
    "    iteration = step_gradient_multi(b, m, data)\n",
    "    b= iteration[0]\n",
    "    m = []\n",
    "    for j in range(len(iteration)):\n",
    "        m.append(iteration[1][j])\n",
    "    iterations.append(iteration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the last step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.944428332442866, array([2.995890, -3.911055]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterations[499]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level up - optional\n",
    "\n",
    "Try your own gradient descent algorithm on the Boston Housing data set, and compare with the result from scikit learn!\n",
    "Be careful to test on a few continuous variables at first, and see how you perform. Scikit learn has built-in \"regularization\" parameters to make optimization more feasible for many parameters.\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this section, we saw our gradient descent formulas in action.  The core of the gradient descent functions is understanding the two lines: \n",
    "\n",
    "$$ \\frac{dJ}{dm}J(m,b) = -2\\sum_{i = 1}^n x(y_i - (mx_i + b)) = -2\\sum_{i = 1}^n x_i*\\epsilon_i$$\n",
    "$$ \\frac{dJ}{db}J(m,b) = -2\\sum_{i = 1}^n(y_i - (mx_i + b)) = -2\\sum_{i = 1}^n \\epsilon_i $$\n",
    "    \n",
    "Which both look to the errors of the current regression line for our dataset to determine how to update the regression line next.  These formulas came from our cost function, $J(m,b) = \\sum_{i = 1}^n(y_i - (mx_i + b))^2 $, and using the gradient to find the direction of steepest descent.  Translating this into code, and seeing how the regression line continued to improve in alignment with the data, we saw the effectiveness of this technique in practice. Additionally, we saw how you can extend the gradient descent algorithm to multiple predictors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
